{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_data as imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyfolio as pf\n",
    "import matplotlib.pyplot as plt\n",
    "import Alphas101 as Alphas\n",
    "\n",
    "import matplotlib as plt\n",
    "from math import sqrt\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from statistics import stdev\n",
    "from pylab import plot,show\n",
    "from numpy import vstack,array\n",
    "from numpy.random import rand\n",
    "from scipy.cluster.vq import kmeans,vq\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import random\n",
    "\n",
    "'''Algos'''\n",
    "import tslearn\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.clustering import KShape, TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from pyclustering.cluster.clarans import clarans;\n",
    "from pyclustering.utils import timedcall;\n",
    "from sklearn import datasets\n",
    "import operator\n",
    "import calendar\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_kshape(df, n = 252, K = 4):\n",
    "    X = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(df.T.values)\n",
    "\n",
    "    ks = KShape(n_clusters=K, max_iter=100, n_init=100, verbose=0).fit(X)\n",
    "\n",
    "    index = list(range(len(df.columns)))\n",
    "    columns = ['strategies', 'clusters', 'selection']\n",
    "\n",
    "    results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    results['clusters'] = ks.labels_\n",
    "    results['strategies'] = df.columns\n",
    "\n",
    "    sharpeclusters = []\n",
    "    dfaux = df.tail(n)\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp = dfaux.pct_change().iloc[:, l].sum(axis=1, skipna=True)\n",
    "        r = dfexp.mean()\n",
    "        s = stdev(dfexp)\n",
    "\n",
    "        sharpeclusters.append(r / s)\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_kmeans(df, n =2520, K =3):\n",
    "\n",
    "    df = df.tail(n)\n",
    "    returns = df.pct_change().mean()\n",
    "    returns = pd.DataFrame(returns)\n",
    "    returns.columns = ['Returns'] \n",
    "    returns['Volatility'] = df.pct_change().std()\n",
    "\n",
    "    #format the data as a numpy array to feed into the K-Means algorithm\n",
    "    data = np.asarray([np.asarray(returns['Returns']),np.asarray(returns['Volatility'])]).T\n",
    "    \n",
    "    n_samples = 1500\n",
    "    random_state = 170\n",
    "\n",
    "    # Number of clusters\n",
    "    kmeans1 = KMeans(n_clusters=K, random_state=random_state)\n",
    "    kmeans1.fit(data)\n",
    "    #y_pred = kmeans1.fit_predict(X) \n",
    "    \n",
    "    results=pd.DataFrame({'Returns': data[:, 0], \n",
    "                       'Volatility': data[:, 1],\n",
    "                       'ClusterkMeans':kmeans1.labels_,\n",
    "                       'strategies': df.columns,\n",
    "                      })\n",
    "    \n",
    "    \n",
    "    sharpeclusters =[] \n",
    "    for i in range(results['ClusterkMeans'].nunique()):\n",
    "    \n",
    "        l = results.loc[results['ClusterkMeans'] == i].index.values.astype(int).tolist()\n",
    "        dfexp=df.pct_change().iloc[: , l].sum(axis = 1, skipna = True)\n",
    "        r = dfexp.mean()\n",
    "        s = stdev(dfexp)\n",
    "    \n",
    "        sharpeclusters.append(r/s)\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "    \n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['ClusterkMeans'] == selection\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "    \n",
    "    \n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSKmeans - Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_tskmeans_euclidean (df, n =252, K =4, metric = \"euclidean\"):\n",
    "    \n",
    "    #  metric : {“euclidean”, “dtw”, “softdtw”} (default: “euclidean”)\n",
    "    \n",
    "    X = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(df.T.values)\n",
    "\n",
    "    \n",
    "    \n",
    "    km = TimeSeriesKMeans(n_clusters=4, max_iter=200, n_init=200,tol = 1e-8, \\\n",
    "                      metric= metric, verbose=1, random_state=2019).fit(X)\n",
    "\n",
    "    index = list(range(len(df.columns)))\n",
    "    columns = ['strategies', 'clusters', 'selection']\n",
    "\n",
    "    results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    results['clusters'] = km.labels_\n",
    "    results['strategies'] = df.columns\n",
    "\n",
    "    sharpeclusters = []\n",
    "    dfaux = df.tail(n)\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp = dfaux.pct_change().iloc[:, l].sum(axis=1, skipna=True)\n",
    "        r = dfexp.mean()\n",
    "        s = stdev(dfexp)\n",
    "\n",
    "        sharpeclusters.append(r / s)\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSKmeans - dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_tskmeans_dtw (df, n =252, K =4, metric = \"dtw\"):\n",
    "    \n",
    "    #  metric : {“euclidean”, “dtw”, “softdtw”} (default: “euclidean”)\n",
    "    \n",
    "    X = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(df.T.values)\n",
    "\n",
    "    \n",
    "    \n",
    "    km = TimeSeriesKMeans(n_clusters=4, max_iter=200, n_init=200,tol = 1e-8, \\\n",
    "                      metric= metric, verbose=1, random_state=2019).fit(X)\n",
    "\n",
    "    index = list(range(len(df.columns)))\n",
    "    columns = ['strategies', 'clusters', 'selection']\n",
    "\n",
    "    results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    results['clusters'] = km.labels_\n",
    "    results['strategies'] = df.columns\n",
    "\n",
    "    sharpeclusters = []\n",
    "    dfaux = df.tail(n)\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp = dfaux.pct_change().iloc[:, l].sum(axis=1, skipna=True)\n",
    "        r = dfexp.mean()\n",
    "        s = stdev(dfexp)\n",
    "\n",
    "        sharpeclusters.append(r / s)\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSKmeans - softdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_tskmeans_softdtw (df, n =252, K =4, metric = \"softdtw\"):\n",
    "    \n",
    "    #  metric : {“euclidean”, “dtw”, “softdtw”} (default: “euclidean”)\n",
    "    \n",
    "    X = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(df.T.values)\n",
    "\n",
    "    \n",
    "    \n",
    "    km = TimeSeriesKMeans(n_clusters=4, max_iter=200, n_init=200,tol = 1e-8, \\\n",
    "                      metric= metric, verbose=1, random_state=2019).fit(X)\n",
    "\n",
    "    index = list(range(len(df.columns)))\n",
    "    columns = ['strategies', 'clusters', 'selection']\n",
    "\n",
    "    results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    results['clusters'] = km.labels_\n",
    "    results['strategies'] = df.columns\n",
    "\n",
    "    sharpeclusters = []\n",
    "    dfaux = df.tail(n)\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp = dfaux.pct_change().iloc[:, l].sum(axis=1, skipna=True)\n",
    "        r = dfexp.mean()\n",
    "        s = stdev(dfexp)\n",
    "\n",
    "        sharpeclusters.append(r / s)\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarans(df,k=3,v=4):\n",
    "    df['year'] = df['Date'].apply(lambda x: x.split('-')[0])\n",
    "    df2 = df[df['year'].isin(['2017','2018','2019'])]\n",
    "    df2 = df2.drop('year',axis=1)\n",
    "    df2=df2.set_index('Date').tail(365)\n",
    "    df3 = df2.tail(365).transpose()\n",
    "    data = np.array(df3)\n",
    "\n",
    "    data = data.tolist()\n",
    "\n",
    "    #get a glimpse of dataset\n",
    "    #print(\"A peek into the dataset : \",data[:4])\n",
    "\n",
    "\n",
    "    \"\"\"!\n",
    "    @brief Constructor of clustering algorithm CLARANS.\n",
    "    @details The higher the value of maxneighbor, the closer is CLARANS to K-Medoids, and the longer is each search of a local minima.\n",
    "    @param[in] data: Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "    @param[in] number_clusters: amount of clusters that should be allocated.\n",
    "    @param[in] numlocal: the number of local minima obtained (amount of iterations for solving the problem).\n",
    "    @param[in] maxneighbor: the maximum number of neighbors examined.        \n",
    "    \"\"\"\n",
    "    clarans_instance = clarans(data, k, 6, v);\n",
    "\n",
    "    #calls the clarans method 'process' to implement the algortihm\n",
    "    (ticks, result) = timedcall(clarans_instance.process);\n",
    "    print(\"Execution time : \", ticks, \"\\n\");\n",
    "\n",
    "    #returns the clusters \n",
    "    clusters = clarans_instance.get_clusters();\n",
    "\n",
    "    #returns the mediods \n",
    "    medoids = clarans_instance.get_medoids();\n",
    "\n",
    "    dic = {}\n",
    "    for var in range(len(clusters)):\n",
    "        for var2 in clusters[var]:\n",
    "            dic[var2] = var\n",
    "\n",
    "\n",
    "    resultado = sorted(dic.items(), key=operator.itemgetter(0))\n",
    "\n",
    "    dic = {}\n",
    "    for var in resultado:\n",
    "        dic[df3.index[var[0]]] = var[1]\n",
    "\n",
    "    clf = pd.DataFrame.from_dict(dic,orient='index')\n",
    "    clf = clf.reset_index()\n",
    "    results = clf.rename(columns={'index':'strategies',0:'clusters'})\n",
    "\n",
    "    sharpeclusters =[] \n",
    "    Features = df2\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp=Features.pct_change().iloc[: , l].sum(axis = 1, skipna = True)\n",
    "        r = dfexp.mean()\n",
    "        s = dfexp.std()\n",
    "\n",
    "        sharpeclusters.append(r/s)\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estrategias = pd.read_csv('Rentabilidad de estrategias base 100.csv')\n",
    "Estrategias.set_index('Date', inplace = True)\n",
    "Estrategias.index = pd.to_datetime(Estrategias.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_date(year_month,day):\n",
    "    return str(year_month[0])+'-'+str(year_month[1])+'-'+str(day)\n",
    "\n",
    "def get_ret(hist_prices, index, buy = [], sell = []):\n",
    "    pyg_long = 0\n",
    "    pyg_short = 0\n",
    "    \n",
    "    if len(buy) != 0:\n",
    "        for name in buy:\n",
    "            pyg_long += hist_prices[name]['ret'][index]\n",
    "    \n",
    "    if len(sell) != 0:\n",
    "        for name in sell:\n",
    "            pyg_short += -1 * hist_prices[name]['ret'][index]\n",
    "    \n",
    "    return pyg_long + pyg_short\n",
    "\n",
    "symbols = list(pd.read_csv(\"Tickers.txt\", sep=\"\\n\", header=None)[0].values)\n",
    "symbols = ['XLP','XLY','XLE','XLF','XLV','XLI','XLK','XLU']\n",
    "#            'EWA', 'EWC', 'EWG','EWH', 'EWJ', 'EWL', 'EWP', 'EWW', 'EWU', 'EWY', 'EWZ', 'INDA']\n",
    "\n",
    "path_features = \"Features.txt\"\n",
    "features = list(pd.read_csv(path_features, sep=\"\\n\", header=None)[0].values)\n",
    "\n",
    "# comission and bid_ask_spread\n",
    "com = 0.0015\n",
    "BidAskSpread = 0.00016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [x for x in range(2008,2020)]\n",
    "months = [x+1 for x in range(12)]\n",
    "years_months = list(it.product(*[years,months]))\n",
    "ld_month = [calendar.monthrange(x[0],x[1])[1] for x in years_months]\n",
    "Rebalancing_dates = pd.to_datetime([y for y in map(set_date, years_months, ld_month)])\n",
    "\n",
    "Bkt_Size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_data = imp.organizarTodo(symbols,years)\n",
    "hist_feature = imp.organizarTodo(features,years,True)\n",
    "\n",
    "\n",
    "\n",
    "Rebalancing_dates = []\n",
    "\n",
    "curr_month = hist_data['XLP'].index[0].month\n",
    "\n",
    "for i in range(1,len(hist_data['XLP'].index)):\n",
    "    if hist_data['XLP'].index[i].month != curr_month:\n",
    "        Rebalancing_dates.append(hist_data['XLP'].index[i-1])\n",
    "        curr_month = hist_data['XLP'].index[i].month\n",
    "        \n",
    "Rebalancing_dates = pd.to_datetime(Rebalancing_dates)\n",
    "the_start = Rebalancing_dates[Rebalancing_dates >= hist_data['XLP'].index[200]][0]\n",
    "ind = np.logical_and(Rebalancing_dates <= hist_data['XLP'].index[-1], Rebalancing_dates >= the_start)\n",
    "Rebalancing_dates = Rebalancing_dates[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the factors over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for etf,data in hist_data.items():\n",
    "    df = data.copy()\n",
    "    df['ret'] = df['Close'].pct_change()\n",
    "    df['Alpha1'] = 0.0\n",
    "    df['Alpha101'] = 0.0\n",
    "    df['Alpha2'] = 0.0\n",
    "    df['Alpha3'] = 0.0\n",
    "    df['Alpha44'] = 0.0\n",
    "    df['Alpha53'] = 0.0\n",
    "    df['Alpha54'] = 0.0\n",
    "    df['Alpha6'] = 0.0\n",
    "    df['AlphaMAR'] = 0.0\n",
    "    \n",
    "    for i in Rebalancing_dates[Rebalancing_dates >= the_start]:\n",
    "        df['Alpha1'][i] = Alphas.alpha1(data.iloc[df.index <= i,])\n",
    "        df['Alpha101'][i] = Alphas.alpha101(df.iloc[df.index <= i,])\n",
    "        df['Alpha2'][i] = Alphas.alpha2(df.iloc[df.index <= i,])\n",
    "        df['Alpha3'][i] = Alphas.alpha3(df.iloc[df.index <= i,])\n",
    "        df['Alpha44'][i] = Alphas.alpha44(df.iloc[df.index <= i,])\n",
    "        df['Alpha53'][i] = Alphas.alpha53(df.iloc[df.index <= i,])\n",
    "        df['Alpha54'][i] = Alphas.alpha54(df.iloc[df.index <= i,])\n",
    "        df['Alpha6'][i] = Alphas.alpha6(df.iloc[df.index <= i,])\n",
    "        df['AlphaMAR'][i] = Alphas.mar(df.iloc[df.index <= i,])\n",
    "        \n",
    "    hist_data[etf] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Strategies = ['LA1','SA1','LSA1',\n",
    "              'LA101','SA101','LSA101',\n",
    "              'LA2','SA2','LSA2',\n",
    "              'LA3','SA3','LSA3',\n",
    "              'LA44','SA44','LSA44',\n",
    "              'LA53','SA53','LSA53',\n",
    "              'LA54','SA54','LSA54',\n",
    "              'LA6','SA6','LSA6',\n",
    "              'LAMAR','SAMAR','LSAMAR']\n",
    "\n",
    "Strategy_Ret = pd.DataFrame(index = hist_data['XLP'].index[hist_data['XLP'].index >= the_start],\n",
    "                            columns = Strategies, dtype = 'float')\n",
    "\n",
    "LA1_basket = []; SA1_basket = []\n",
    "\n",
    "LA101_basket = []; SA101_basket = []\n",
    "\n",
    "LA2_basket = []; SA2_basket = []\n",
    "\n",
    "LA3_basket = []; SA3_basket = []\n",
    "\n",
    "LA44_basket = []; SA44_basket = []\n",
    "\n",
    "LA53_basket = []; SA53_basket = []\n",
    "\n",
    "LA54_basket = []; SA54_basket = []\n",
    "\n",
    "LA6_basket = []; SA6_basket = []\n",
    "\n",
    "LAMAR_basket = []; SAMAR_basket = []\n",
    "\n",
    "    \n",
    "for i in Strategy_Ret.index:\n",
    "            \n",
    "    if i in Rebalancing_dates:\n",
    "        \n",
    "        # update Baskets\n",
    "        \n",
    "        LA1_basket = pd.DataFrame({k:d['Alpha1'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA1_basket = list(LA1_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA1_basket = pd.DataFrame({k:d['Alpha1'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA1_basket = list(SA1_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA101_basket = pd.DataFrame({k:d['Alpha101'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA101_basket = list(LA101_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA101_basket = pd.DataFrame({k:d['Alpha101'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA101_basket = list(SA101_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA2_basket = pd.DataFrame({k:d['Alpha2'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA2_basket = list(LA2_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA2_basket = pd.DataFrame({k:d['Alpha2'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA2_basket = list(SA2_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA3_basket = pd.DataFrame({k:d['Alpha3'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA3_basket = list(LA3_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA3_basket = pd.DataFrame({k:d['Alpha3'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA3_basket = list(SA3_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA44_basket = pd.DataFrame({k:d['Alpha44'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA44_basket = list(LA44_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA44_basket = pd.DataFrame({k:d['Alpha44'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA44_basket = list(SA44_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA53_basket = pd.DataFrame({k:d['Alpha53'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA53_basket = list(LA53_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA53_basket = pd.DataFrame({k:d['Alpha53'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA53_basket = list(SA53_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA54_basket = pd.DataFrame({k:d['Alpha54'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA54_basket = list(LA54_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA54_basket = pd.DataFrame({k:d['Alpha54'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA54_basket = list(SA54_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        LA6_basket = pd.DataFrame({k:d['Alpha6'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LA6_basket = list(LA6_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SA6_basket = pd.DataFrame({k:d['Alpha6'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SA6_basket = list(SA6_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "                \n",
    "        LAMAR_basket = pd.DataFrame({k:d['AlphaMAR'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        LAMAR_basket = list(LAMAR_basket.sort_values(by=['0'],ascending=False).index[0:Bkt_Size])\n",
    "        \n",
    "        SAMAR_basket = pd.DataFrame({k:d['AlphaMAR'][i] for k,d in hist_data.items()},index=['0']).transpose()\n",
    "        SAMAR_basket = list(SAMAR_basket.sort_values(by=['0']).index[0:Bkt_Size])\n",
    "        \n",
    "        # Return calculus\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA1']] = get_ret(hist_data, i, buy= LA1_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA1']] = get_ret(hist_data, i, sell= SA1_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA1']] = get_ret(hist_data, i, buy= LA1_basket, sell= SA1_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA101']] = get_ret(hist_data, i, buy= LA101_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA101']] = get_ret(hist_data, i, sell= SA101_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA101']] = get_ret(hist_data, i, buy= LA101_basket, sell= SA101_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA2']] = get_ret(hist_data, i, buy= LA2_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA2']] = get_ret(hist_data, i, sell= SA2_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA2']] = get_ret(hist_data, i, buy= LA2_basket, sell = SA2_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA3']] = get_ret(hist_data, i, buy= LA3_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA3']] = get_ret(hist_data, i, sell= SA3_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA3']] = get_ret(hist_data, i, buy= LA3_basket, sell= SA3_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA44']] = get_ret(hist_data, i, buy= LA44_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA44']] = get_ret(hist_data, i, sell= SA44_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA44']] = get_ret(hist_data, i, buy= LA44_basket, sell= SA44_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA53']] = get_ret(hist_data, i, buy= LA53_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA53']] = get_ret(hist_data, i, sell= SA53_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA53']] = get_ret(hist_data, i, buy= LA53_basket, sell= SA53_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA54']] = get_ret(hist_data, i, buy= LA54_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA54']] = get_ret(hist_data, i, sell= SA54_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA54']] = get_ret(hist_data, i, buy= LA54_basket, sell= SA54_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA6']] = get_ret(hist_data, i, buy= LA6_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA6']] = get_ret(hist_data, i, sell= SA6_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA6']] = get_ret(hist_data, i, buy= LA6_basket, sell= SA6_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LAMAR']] = get_ret(hist_data, i, buy= LAMAR_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SAMAR']] = get_ret(hist_data, i, sell= SAMAR_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSAMAR']] = get_ret(hist_data, i, buy= LAMAR_basket, sell= SAMAR_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA1']] = get_ret(hist_data, i, buy= LA1_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA1']] = get_ret(hist_data, i, sell= SA1_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA1']] = get_ret(hist_data, i, buy= LA1_basket, sell= SA1_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA101']] = get_ret(hist_data, i, buy= LA101_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA101']] = get_ret(hist_data, i, sell= SA101_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA101']] = get_ret(hist_data, i, buy= LA101_basket, sell= SA101_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA2']] = get_ret(hist_data, i, buy= LA2_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA2']] = get_ret(hist_data, i, sell= SA2_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA2']] = get_ret(hist_data, i, buy= LA2_basket, sell = SA2_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA3']] = get_ret(hist_data, i, buy= LA3_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA3']] = get_ret(hist_data, i, sell= SA3_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA3']] = get_ret(hist_data, i, buy= LA3_basket, sell= SA3_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA44']] = get_ret(hist_data, i, buy= LA44_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA44']] = get_ret(hist_data, i, sell= SA44_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA44']] = get_ret(hist_data, i, buy= LA44_basket, sell= SA44_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA53']] = get_ret(hist_data, i, buy= LA53_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA53']] = get_ret(hist_data, i, sell= SA53_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA53']] = get_ret(hist_data, i, buy= LA53_basket, sell = SA53_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA54']] = get_ret(hist_data, i, buy= LA54_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA54']] = get_ret(hist_data, i, sell= SA54_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA54']] = get_ret(hist_data, i, buy= LA54_basket, sell= SA54_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LA6']] = get_ret(hist_data, i, buy= LA6_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SA6']] = get_ret(hist_data, i, sell= SA6_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSA6']] = get_ret(hist_data, i, buy= LA6_basket, sell= SA6_basket)*(1-com)*(1-BidAskSpread)\n",
    "        \n",
    "        Strategy_Ret.loc[i,['LAMAR']] = get_ret(hist_data, i, buy= LAMAR_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['SAMAR']] = get_ret(hist_data, i, sell= SAMAR_basket)*(1-com)*(1-BidAskSpread)\n",
    "        Strategy_Ret.loc[i,['LSAMAR']] = get_ret(hist_data, i, buy= LAMAR_basket, sell= SAMAR_basket)*(1-com)*(1-BidAskSpread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Strategy_Prices_TS = 100 * (Strategy_Ret+1).cumprod()\n",
    "Strategy_Prices_TS.to_csv('Rentabilidad de estrategias base 100.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
