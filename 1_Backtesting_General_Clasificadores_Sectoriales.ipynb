{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pyfolio\\pos.py:27: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  'Module \"zipline.assets\" not found; mutltipliers will not be applied' +\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tslearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-728cd0ef2fa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;34m'''Algos'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtslearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtslearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_time_series_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtslearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclustering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKShape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimeSeriesScalerMeanVariance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tslearn'"
     ]
    }
   ],
   "source": [
    "import read_data as imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyfolio as pf\n",
    "import matplotlib.pyplot as plt\n",
    "import Alphas101 as Alphas\n",
    "\n",
    "import matplotlib as plt\n",
    "from math import sqrt\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from statistics import stdev\n",
    "from pylab import plot,show\n",
    "from numpy import vstack,array\n",
    "from numpy.random import rand\n",
    "from scipy.cluster.vq import kmeans,vq\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import random\n",
    "\n",
    "'''Algos'''\n",
    "import tslearn\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.clustering import KShape, TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from pyclustering.cluster.clarans import clarans;\n",
    "from pyclustering.utils import timedcall;\n",
    "from sklearn import datasets\n",
    "import operator\n",
    "import calendar\n",
    "import itertools as it\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(points):\n",
    "    n = points.shape[0]\n",
    "    distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[i,j] = np.linalg.norm(points[i,:] - points[j,:])\n",
    "    return distances\n",
    "\n",
    "def cluster(distances, k=3):\n",
    "\n",
    "    m = distances.shape[0] # number of points\n",
    "\n",
    "    # Pick k random medoids.\n",
    "    curr_medoids = np.array([-1]*k)\n",
    "    while not len(np.unique(curr_medoids)) == k:\n",
    "        curr_medoids = np.array([random.randint(0, m - 1) for _ in range(k)])\n",
    "    old_medoids = np.array([-1]*k) # Doesn't matter what we initialize these to.\n",
    "    new_medoids = np.array([-1]*k)\n",
    "\n",
    "    # Until the medoids stop updating, do the following:\n",
    "    while not ((old_medoids == curr_medoids).all()):\n",
    "        # Assign each point to cluster with closest medoid.\n",
    "        clusters = assign_points_to_clusters(curr_medoids, distances)\n",
    "\n",
    "        # Update cluster medoids to be lowest cost point.\n",
    "        for curr_medoid in curr_medoids:\n",
    "            cluster = np.where(clusters == curr_medoid)[0]\n",
    "            new_medoids[curr_medoids == curr_medoid] = compute_new_medoid(cluster, distances)\n",
    "\n",
    "        old_medoids[:] = curr_medoids[:]\n",
    "        curr_medoids[:] = new_medoids[:]\n",
    "\n",
    "    return clusters, curr_medoids\n",
    "\n",
    "def assign_points_to_clusters(medoids, distances):\n",
    "    distances_to_medoids = distances[:,medoids]\n",
    "    clusters = medoids[np.argmin(distances_to_medoids, axis=1)]\n",
    "    assert (clusters[medoids] == medoids).all()\n",
    "    return clusters\n",
    "\n",
    "def compute_new_medoid(cluster, distances):\n",
    "    mask = np.ones(distances.shape)\n",
    "    mask[np.ix_(cluster,cluster)] = 0.\n",
    "    cluster_distances = np.ma.masked_array(data=distances, mask=mask, fill_value=10e9)\n",
    "    costs = cluster_distances.sum(axis=1)\n",
    "    return costs.argmin(axis=0, fill_value=10e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_kshape(df, K = 4):\n",
    "    X = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(df.T.values)[:,:,0]\n",
    "    ks = KShape(n_clusters=K, max_iter=100, n_init=100, verbose=0).fit(X)\n",
    "\n",
    "    index = list(range(len(df.columns)))\n",
    "    columns = ['strategies', 'clusters', 'selection']\n",
    "    results = pd.DataFrame(index=index, columns=columns)\n",
    "    results['clusters'] = ks.labels_\n",
    "    results['strategies'] = df.columns\n",
    "\n",
    "    sharpeclusters = []\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp = df.iloc[:, l].sum(axis=1, skipna=True)/len(l)\n",
    "        sharpeclusters.append(pf.timeseries.perf_stats(dfexp)['Sharpe ratio'])\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_kmeans(df, K =3):\n",
    "\n",
    "    data = pd.DataFrame(columns= df.columns)\n",
    "    for col in df.columns:\n",
    "        data[col] = pf.timeseries.perf_stats(df[col])\n",
    "    data = data.T.values\n",
    "\n",
    "    n_samples = 1500\n",
    "    random_state = 170\n",
    "\n",
    "    kmeans1 = KMeans(n_clusters=K, random_state=random_state)\n",
    "    kmeans1.fit(data)\n",
    "    \n",
    "    results=pd.DataFrame({'Returns': data[:, 0], \n",
    "                       'Volatility': data[:, 2],\n",
    "                       'ClusterkMeans':kmeans1.labels_,\n",
    "                       'strategies': df.columns,\n",
    "                      })\n",
    "    \n",
    "    sharpeclusters =[] \n",
    "    for i in range(results['ClusterkMeans'].nunique()):\n",
    "        l = results.loc[results['ClusterkMeans'] == i].index.values.astype(int).tolist()\n",
    "        dfexp=df.iloc[:,l].sum(axis = 1, skipna = True)/len(l)\n",
    "        sharpeclusters.append(pf.timeseries.perf_stats(dfexp)['Sharpe ratio'])\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['ClusterkMeans'] == selection\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "        \n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSKmeans - Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_tskmeans_euclidean (df, K =4, metric = \"euclidean\"):\n",
    "   \n",
    "    X = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(df.T.values)\n",
    "    km = TimeSeriesKMeans(n_clusters=4, max_iter=200, n_init=200,tol = 1e-8,\n",
    "                          metric= metric, verbose=0, random_state=2019).fit(X)\n",
    "\n",
    "    index = list(range(len(df.columns)))\n",
    "    columns = ['strategies', 'clusters', 'selection']\n",
    "\n",
    "    results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    results['clusters'] = km.labels_\n",
    "    results['strategies'] = df.columns\n",
    "\n",
    "    sharpeclusters = []\n",
    "    for i in range(results['clusters'].nunique()):\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp = df.iloc[:, l].sum(axis=1, skipna=True)/len(l)\n",
    "        sharpeclusters.append(pf.timeseries.perf_stats(dfexp)['Sharpe ratio'])\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_clarans(df,k=3,v=4):\n",
    "    df2 = df.transpose()\n",
    "    data = np.array(df2)\n",
    "    data = data.tolist()\n",
    "\n",
    "    clarans_instance = clarans(data, k, 6, v);\n",
    "    #calls the clarans method 'process' to implement the algortihm\n",
    "    (ticks, result) = timedcall(clarans_instance.process);\n",
    "\n",
    "    #returns the clusters \n",
    "    clusters = clarans_instance.get_clusters();\n",
    "\n",
    "    #returns the mediods \n",
    "    medoids = clarans_instance.get_medoids();\n",
    "\n",
    "    dic = {}\n",
    "    for var in range(len(clusters)):\n",
    "        for var2 in clusters[var]:\n",
    "            dic[var2] = var\n",
    "\n",
    "\n",
    "    resultado = sorted(dic.items(), key=operator.itemgetter(0))\n",
    "\n",
    "    dic = {}\n",
    "    for var in resultado:\n",
    "        dic[df2.index[var[0]]] = var[1]\n",
    "\n",
    "    clf = pd.DataFrame.from_dict(dic,orient='index')\n",
    "    clf = clf.reset_index()\n",
    "    results = clf.rename(columns={'index':'strategies',0:'clusters'})\n",
    "\n",
    "    sharpeclusters =[] \n",
    "    for i in range(results['clusters'].nunique()):\n",
    "\n",
    "        l = results.loc[results['clusters'] == i].index.values.astype(int).tolist()\n",
    "        dfexp=df.iloc[: , l].sum(axis = 1, skipna = True)/len(l)\n",
    "        \n",
    "        sharpeclusters.append(pf.timeseries.perf_stats(dfexp)['Sharpe ratio'])\n",
    "    sharpeclusters = np.asanyarray(sharpeclusters)\n",
    "\n",
    "    selection = sharpeclusters.argmax()\n",
    "    cond = results['clusters'] == selection\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)\n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_kmedoids(df, K = 3):\n",
    "    \n",
    "    data = pd.DataFrame(columns= df.columns)\n",
    "    for col in df.columns:\n",
    "        data[col] = pf.timeseries.perf_stats(df[col])\n",
    "    data = data.T.values\n",
    "\n",
    "    distances = compute_distances(data)    \n",
    "\n",
    "    _clusters, medoids = cluster(distances, K)\n",
    "\n",
    "    results=pd.DataFrame({'Returns': data[:, 0], \n",
    "                   'Volatility': data[:, 1],\n",
    "                   'ClusterkMedoids':_clusters,\n",
    "                   'strategies': df.columns,\n",
    "                  })\n",
    "\n",
    "    medoidsclusters =[] \n",
    "\n",
    "    for i in results.ClusterkMedoids.unique():\n",
    "        l = results.loc[results['ClusterkMedoids'] == i].index.values.astype(int).tolist()\n",
    "        dfexp=df.iloc[: , l].sum(axis = 1, skipna = True)/len(l)\n",
    "        medoidsclusters.append(pf.timeseries.perf_stats(dfexp)['Sharpe ratio'])\n",
    "    medoidsclusters = np.asanyarray(medoidsclusters)    \n",
    "\n",
    "    selection = medoidsclusters.argmax()\n",
    "\n",
    "    cond = results['ClusterkMedoids'] == results.ClusterkMedoids.unique()[selection]\n",
    "\n",
    "    results['selection'] = np.where(cond, 1, 0)    \n",
    "\n",
    "    return results['strategies'][results['selection'] == 1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estrategias = pd.read_csv('Rentabilidad de estrategias Sectoriales base 100.csv')\n",
    "Estrategias.set_index('Date', inplace = True)\n",
    "Estrategias.index = pd.to_datetime(Estrategias.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Initial_Date = pd.to_datetime('2014-01-01')\n",
    "\n",
    "Rebalancing_dates = []\n",
    "curr_month = Initial_Date.month\n",
    "\n",
    "the_start = np.where(Initial_Date == Estrategias.index)[0].tolist()[0]\n",
    "\n",
    "for i in range((the_start+1), len(Estrategias.index)):\n",
    "    if Estrategias.index[i].month != curr_month:\n",
    "        Rebalancing_dates.append(Estrategias.index[i-1])\n",
    "        curr_month = Estrategias.index[i].month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CÃ¡lculo de la rentabilidad de los retornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering_Returns = pd.DataFrame(index = Estrategias.index[Estrategias.index >= Initial_Date],\n",
    "                                  columns = ['Kshape', 'Kmeans', 'TSKM-E', 'Clarans', 'Kmedoids'])\n",
    "\n",
    "Kshape = []\n",
    "Kmeans = []\n",
    "TSKME = []\n",
    "Clarans = []\n",
    "Kmedoids = []\n",
    "\n",
    "Memory_time_window = 126\n",
    "\n",
    "for i in Estrategias.index[Estrategias.index >= Initial_Date]:\n",
    "            \n",
    "    if i in Rebalancing_dates:\n",
    "        \n",
    "        end = np.where(i == Estrategias.index)[0].tolist()[0]\n",
    "        ini = end - Memory_time_window\n",
    "        \n",
    "        # update Baskets\n",
    "        \n",
    "        Kshape = strategy_kshape(Estrategias.iloc[ini:end,:])\n",
    "        Kmeans = strategy_kmeans(Estrategias.iloc[ini:end,:])\n",
    "        TSKME = strategy_tskmeans_euclidean(Estrategias.iloc[ini:end,:])\n",
    "        Clarans = strategy_clarans(Estrategias.iloc[ini:end,:])\n",
    "        Kmedoids = strategy_kmedoids(Estrategias.iloc[ini:end,:])\n",
    "        \n",
    "        # Return calculus\n",
    "        \n",
    "        Clustering_Returns.loc[i,'Kshape'] = Estrategias.loc[i,Kshape].sum()/len(Kshape)\n",
    "        Clustering_Returns.loc[i,'Kmeans'] = Estrategias.loc[i,Kmeans].sum()/len(Kmeans)\n",
    "        Clustering_Returns.loc[i,'TSKM-E'] = Estrategias.loc[i,TSKME].sum()/len(TSKME)\n",
    "        Clustering_Returns.loc[i,'Clarans'] = Estrategias.loc[i,Clarans].sum()/len(Clarans)\n",
    "        Clustering_Returns.loc[i,'Kmedoids'] = Estrategias.loc[i,Kmedoids].sum()/len(Kmedoids)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        Clustering_Returns.loc[i,'Kshape'] = Estrategias.loc[i,Kshape].sum()/len(Kshape)\n",
    "        Clustering_Returns.loc[i,'Kmeans'] = Estrategias.loc[i,Kmeans].sum()/len(Kmeans)\n",
    "        Clustering_Returns.loc[i,'TSKM-E'] = Estrategias.loc[i,TSKME].sum()/len(TSKME)\n",
    "        Clustering_Returns.loc[i,'Clarans'] = Estrategias.loc[i,Clarans].sum()/len(Clarans)\n",
    "        Clustering_Returns.loc[i,'Kmedoids'] = Estrategias.loc[i,Kmedoids].sum()/len(Kmedoids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering_Returns_TS = 100 * (Clustering_Returns+1).cumprod()\n",
    "Clustering_Returns.to_csv('Cluster Rent Sectoriales Base 100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Clustering_Returns_TS.plot(figsize=(10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
